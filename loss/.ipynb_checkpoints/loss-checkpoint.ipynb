{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f7ef2c",
   "metadata": {},
   "source": [
    "# 机器学习中的loss总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c98d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f43be5",
   "metadata": {},
   "source": [
    "## 分类loss\n",
    "\n",
    "### entropy\n",
    "* 动机 \\\n",
    "熵度量分布p的不确定程度，分布越不确定，编码所需的bit数越多。比如: \\\n",
    "[$\\frac{1}{4}$, $\\frac{1}{4}$, $\\frac{1}{4}$, $\\frac{1}{4}$] 比 [$\\frac{1}{2}$, $\\frac{1}{2}$]分布复杂，前者熵值为2，需要2个bits进行编码，后者熵值为1，需要1个bit即可。\n",
    "* 定义\\\n",
    "$$\n",
    "H(p) = -\\sum_{i}p_ilog_2(p_i)\n",
    "$$\n",
    "\n",
    "### cross entropy\n",
    "* 动机\\\n",
    "交叉熵度量的是,分布q与分布p的接近程度,用分布q去逼近分布p，分布q与分布p越接近，交叉熵越小，反之，越大。\n",
    "* 定义\\\n",
    "$$\n",
    "H(p,q) = -\\sum_{i}p_ilog_2(q_i)\n",
    "$$\n",
    "\n",
    "### KL divergence\n",
    "* 动机\\\n",
    "度量概率分布之间的距离\\\n",
    "KL散度，又叫做相对熵，分布q与分布p之间的接近程度减去分布p自身的混乱程度。\n",
    "* 定义\\\n",
    "$$\\begin{align}\n",
    "D_{KL}(p,q) &= \\sum_{i}p_ilog_2(\\frac{p_i}{q_i})\\\\\n",
    "         &= \\sum_{i}p_i\\left[log_2(p_i)-log_2(q_i)\\right]\\\\\n",
    "         &= \\sum_{i}p_ilog_2(p_i)-\\sum_{i}p_ilog_2(q_i)\\\\\n",
    "         &= -H(p) + H(p,q) \\\\\n",
    "         &= H(p,q) - H(p)\n",
    "\\end{align}\n",
    "$$\n",
    "intuitive case: https://www.youtube.com/watch?v=SxGYPqCgJWM \\\n",
    "* 例子\\\n",
    "有两枚硬币1和2：\n",
    "$$\n",
    "coin_1 = \\begin{cases}\n",
    "p_1,  & head 正面 \\\\\n",
    "p_2, & tail 反面\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "coin_2 = \\begin{cases}\n",
    "q_1,  & head 正面 \\\\\n",
    "q_2, & tail 反面\n",
    "\\end{cases}\n",
    "$$\n",
    "观察到的抛硬币的序列：$[H, H, T, H, T, H, H, T, H,T, T, H]$\n",
    "$$\n",
    "P(observations|coin_1) = p_1^{N_h}p_2^{N_t}\\\\\n",
    "P(observations|coin_2) = q_1^{N_h}q_2^{N_t}\\\\\n",
    "\\frac{P(observations|coin_1)}{P(observations|coin_2)} = \\frac{p_1^{N_h}p_2^{N_t}}{q_1^{N_h}q_2^{N_t}} \\\\\n",
    "\\frac{1}{N}log(\\frac{P(observations|coin_1)}{P(observations|coin_2)}) = \\frac{1}{N}log(\\frac{p_1^{N_h}p_2^{N_t}}{q_1^{N_h}q_2^{N_t}})\\\\\n",
    "\\frac{1}{N}log(\\frac{P(observations|coin_1)}{P(observations|coin_2)}) = \\frac{1}{N}\\left[log(p_1^{N_h}) + log(p_2^{N_t})-log(q_1^{N_h}) - log(q_2^{N_t})\\right]\\\\\n",
    "\\frac{1}{N}log(\\frac{P(observations|coin_1)}{P(observations|coin_2)}) = \\frac{N_h}{N}log(p_1) + \\frac{N_t}{N}log(p_2)-\\frac{N_h}{N}log(q_1) - \\frac{N_t}{N}log(q_2)\\\\\n",
    "\\lim_{N \\to \\infty}\\frac{1}{N}log(\\frac{P(observations|coin_1)}{P(observations|coin_2)}) = p_1log(p_1) + p_2log(p_2)-p_1log(q_1) - p_2log(q_2)\\\\\n",
    "\\lim_{N \\to \\infty}\\frac{1}{N}log(\\frac{P(observations|coin_1)}{P(observations|coin_2)}) = p_1log(p_1)-p_1log(q_1) + p_2log(p_2) - p_2log(q_2)\\\\\n",
    "\\lim_{N \\to \\infty}\\frac{1}{N}log(\\frac{P(observations|coin_1)}{P(observations|coin_2)}) = p_1log(\\frac{p_1}{q_1}) + p_2log(\\frac{p_2}{q2})\\\\\n",
    "$$\n",
    "拓展到多分类：\n",
    "$$\n",
    "D_{KL}(p,q) = \\sum_{i}p_ilog_2(\\frac{p_i}{q_i})\n",
    "$$\n",
    "* 应用\n",
    "    * R_Drop：https://github.com/dropreg/R-Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a60a0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23c4b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真实分布y: [0.1, 0.2, 0.7]\n",
    "# 预测分布y_hat: [0.1, 0.2, 0.7]\n",
    "# 也就是说预测分布万全接近真实分布\n",
    "# 1) 计算cross_entropy\n",
    "H_y_yhat = -(0.1*np.log2(0.1) + 0.2*np.log2(0.2) + 0.7*np.log2(0.7)) # 1.15\n",
    "\n",
    "# 2) 计算分布p的自身熵值\n",
    "H_y = -(0.1*np.log2(0.1) + 0.2*np.log2(0.2) + 0.7*np.log2(0.7)) # 1.15\n",
    "\n",
    "# 3) 计算y与y_hat之间的 DL divergence\n",
    "D_kl_y_yhat = 0.1*np.log2(0.1/0.1) + 0.2*np.log2(0.2/0.2) + 0.7*np.log2(0.7/0.7) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3bb74",
   "metadata": {},
   "source": [
    "上面的例子说明：\\\n",
    "预测分布与真实分布的交叉熵值比较大，可能是真实分布自身比较复杂引起的，预测分布与真实分布之间的KL散度更能反应两个分布之间的接近程度。\\\n",
    "当真实分布是one-hot分布时，预测分布与真实分布越接近，交叉熵越接近0，反之越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_Drop\n",
    "# 核心思想：sub-model dropout两次之后的输入分布需要保持一致，比如一个句子经过ernie+dropout之后的表示应该保持一致，用KL散度来做惩罚进行正则化。\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define your task model, which outputs the classifier logits\n",
    "model = TaskModel()\n",
    "\n",
    "def compute_kl_loss(self, p, q, pad_mask=None):\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='none')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='none')\n",
    "    # pad_mask is for seq-level tasks\n",
    "    if pad_mask is not None:\n",
    "        p_loss.masked_fill_(pad_mask, 0.)\n",
    "        q_loss.masked_fill_(pad_mask, 0.)\n",
    "    # You can choose whether to use function \"sum\" and \"mean\" depending on your task\n",
    "    p_loss = p_loss.sum()\n",
    "    q_loss = q_loss.sum()\n",
    "    loss = (p_loss + q_loss) / 2\n",
    "    return loss\n",
    "\n",
    "# keep dropout and forward twice\n",
    "logits = model(x)\n",
    "logits2 = model(x)\n",
    "\n",
    "# cross entropy loss for classifier\n",
    "ce_loss = 0.5 * (cross_entropy_loss(logits, label) + cross_entropy_loss(logits2, label))\n",
    "kl_loss = compute_kl_loss(logits, logits2)\n",
    "\n",
    "# carefully choose hyper-parameters\n",
    "loss = ce_loss + α * kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5dc1c5",
   "metadata": {},
   "source": [
    "## 排序loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83378401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1878c9b9",
   "metadata": {},
   "source": [
    "## reduction 选 sum or mean\n",
    "各种loss中都有一个reduction参数，一般有三个选项: None, sum, mean，这个参数的含义是对一个batch的每个样本对于的loss进行求和或者取平均，默认是取平均。取平均可以规避对batch size的依赖，从而不需要根据batch_size的设定来调整learning_rate的大小。看下数学分析:\\\n",
    "比如，MSEloss\n",
    "\n",
    "$$\n",
    "Loss=\\begin{cases}\n",
    "\\sum_{i=1}^N (\\hat{y_i}-y_i)^2,  & reduction=sum \\\\\n",
    "\\frac{1}{N}\\sum_{i=1}^N (\\hat{y_i}-y_i)^2, & reduction=mean\n",
    "\\end{cases}\n",
    "$$\n",
    "其中batch_size=N, $\\hat{y_i}=f(x_i)$, $x_i$是第 $i$ 个样本，f($\\cdot$)是模型。\\\n",
    "求偏导\\\n",
    "$$\n",
    "\\frac{\\partial{Loss}}{\\partial{X}}=\\begin{cases}\n",
    "\\sum_{i=1}^N 2*(\\hat{y_i}-y_i)*\\frac{\\partial{\\hat{y_i}}}{\\partial{x_i}},  & reduction=sum \\\\\n",
    "\\frac{1}{N}\\sum_{i=1}^N 2*(\\hat{y_i}-y_i)*\\frac{\\partial{\\hat{y_i}}}{\\partial{x_i}}, & reduction=mean\n",
    "\\end{cases}\n",
    "$$\n",
    "可以看出，reduction=sum时，偏导会受batch_size影响,当reduction=mean时，偏导基本不受batch_size影响，因为平均之后，均值基本偏差不大。所以默认的reduction选mean。\\\n",
    "代码验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fa88429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [8.65152359])\n",
      "Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [1107.39501953])\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "batch_size=128\n",
    "feature_num = 10\n",
    "model = nn.Linear(feature_num, 1)\n",
    "x = paddle.randn([batch_size, feature_num])\n",
    "y = paddle.randn([batch_size, 1])\n",
    "\n",
    "# mean\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "loss.backward()\n",
    "print(model.weight.grad.abs().sum())\n",
    "# batch_size=10 跑10次\n",
    "# 12.3  12.4  6.8  5.9  14  6  9  9  5.9  23.6\n",
    "# batch_size=128 跑10次\n",
    "# 9   9   4.8   10.7   5.7  10  8.4  7.4  8.6\n",
    "\n",
    "# sum\n",
    "model.clear_gradients()\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "loss.backward()\n",
    "print(model.weight.grad.abs().sum())\n",
    "# batch_size=10 跑10次 \n",
    "# 122.9  123.6  68  58.9  143  61  91  90  58.6  236.6\n",
    "# batch_size=128 跑10次\n",
    "# 1160   1182   619   1380  732  1366.7  1076  956  1107"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361420d8",
   "metadata": {},
   "source": [
    "通过上面的实验可以看出:\\\n",
    "<font color=red>**reduction=mean 可以使loss不受batch_size变化的影响，使梯度的更新更稳定，进而不需根据batch_size调整learning_rate**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a1d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
