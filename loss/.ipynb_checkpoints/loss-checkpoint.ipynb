{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f7ef2c",
   "metadata": {},
   "source": [
    "# 机器学习中的loss总结\n",
    "\n",
    "## 分类loss\n",
    "\n",
    "## 排序loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c98d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878c9b9",
   "metadata": {},
   "source": [
    "## reduction 选 sum or mean\n",
    "各种loss中都有一个reduction参数，一般有三个选项: None, sum, mean，这个参数的含义是对一个batch的每个样本对于的loss进行求和或者取平均，默认是取平均。取平均可以规避对batch size的依赖，从而不需要根据batch_size的设定来调整learning_rate的大小。看下数学分析:\\\n",
    "比如，MSEloss\n",
    "\n",
    "$$\n",
    "Loss=\\begin{cases}\n",
    "\\sum_{i=1}^N (\\hat{y_i}-y_i)^2,  & reduction=sum \\\\\n",
    "\\frac{1}{N}\\sum_{i=1}^N (\\hat{y_i}-y_i)^2, & reduction=mean\n",
    "\\end{cases}\n",
    "$$\n",
    "其中batch_size=N, $\\hat{y_i}=f(x_i)$, $x_i$是第 $i$ 个样本，f($\\cdot$)是模型。\\\n",
    "求偏导\\\n",
    "$$\n",
    "\\frac{\\partial{Loss}}{\\partial{X}}=\\begin{cases}\n",
    "\\sum_{i=1}^N 2*(\\hat{y_i}-y_i)*\\frac{\\partial{\\hat{y_i}}}{\\partial{x_i}},  & reduction=sum \\\\\n",
    "\\frac{1}{N}\\sum_{i=1}^N 2*(\\hat{y_i}-y_i)*\\frac{\\partial{\\hat{y_i}}}{\\partial{x_i}}, & reduction=mean\n",
    "\\end{cases}\n",
    "$$\n",
    "可以看出，reduction=sum时，偏导会受batch_size影响,当reduction=mean时，偏导基本不受batch_size影响，因为平均之后，均值基本偏差不大。所以默认的reduction选mean。\\\n",
    "代码验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27ea4636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [8.65152359])\n",
      "Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [1107.39501953])\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "batch_size=128\n",
    "feature_num = 10\n",
    "model = nn.Linear(feature_num, 1)\n",
    "x = paddle.randn([batch_size, feature_num])\n",
    "y = paddle.randn([batch_size, 1])\n",
    "\n",
    "# mean\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "loss.backward()\n",
    "print(model.weight.grad.abs().sum())\n",
    "# batch_size=10 跑10次\n",
    "# 12.3  12.4  6.8  5.9  14  6  9  9  5.9  23.6\n",
    "# batch_size=128 跑10次\n",
    "# 9   9   4.8   10.7   5.7  10  8.4  7.4  8.6\n",
    "\n",
    "# sum\n",
    "model.clear_gradients()\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "loss.backward()\n",
    "print(model.weight.grad.abs().sum())\n",
    "# batch_size=10 跑10次 \n",
    "# 122.9  123.6  68  58.9  143  61  91  90  58.6  236.6\n",
    "# batch_size=128 跑10次\n",
    "# 1160   1182   619   1380  732  1366.7  1076  956  1107"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18f964",
   "metadata": {},
   "source": [
    "通过上面的实验可以看出:\\\n",
    "<font color=red>**reduction=mean 可以使loss不受batch_size变化的影响，使梯度的更新更稳定，进而不需根据batch_size调整learning_rate**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b9360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
